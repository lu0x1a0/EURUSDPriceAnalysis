{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from DataManipulation.DataHandler import getOHLC_pickle\n",
    "data = getOHLC_pickle(\"EURUSD_M_2010_2021.pkl\")\n",
    "data = data.resample('1H').agg({'Open': 'first', \n",
    "                        'High': 'max', \n",
    "                        'Low': 'min', \n",
    "                        'Close': 'last'}).dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#generates the indicators\n",
    "from DataManipulation.indicators import DEMA, D1, MaxMinRollNorm\n",
    "demaperiod = [9]\n",
    "for p in demaperiod:\n",
    "    data['dema'+str(p)] = DEMA(data['Close'],p)\n",
    "    data['D1dema'+str(p)] = D1(data['dema'+str(p)])\n",
    "#    data['normD1dema'+str(p)] = MaxMinRollNorm(data['D1dema'+str(p)], 24*250) # 250 approximated larger than all trading days in a year\n",
    "\n",
    "stdperiod = [9,100,300]\n",
    "for p in stdperiod:\n",
    "    data['std'+str(p)+\"_dema9\"]  = data['dema9'].rolling(p).std()\n",
    "    data['std'+str(p)+\"_D1dema9\"]= data['D1dema9'].rolling(p).std()\n",
    "print(data.columns)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['Open', 'High', 'Low', 'Close', 'dema9', 'D1dema9', 'std9_dema9',\n",
      "       'std9_D1dema9', 'std100_dema9', 'std100_D1dema9', 'std300_dema9',\n",
      "       'std300_D1dema9'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#x = pd.Series([np.nan,1,2,3,4,5,6,7,8,9])\n",
    "#y = pd.Series([np.nan,2,3,4,5,6,7,8,9,10])\n",
    "#print((x[1:]-y[:-1]).shape)\n",
    "#print((x[1:]-y[:-1]))\n",
    "#print((x[1:].to_numpy()-y[:-1].to_numpy()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "prediction_ind = ['D1dema9','std9_dema9','std9_D1dema9', 'std100_dema9', 'std100_D1dema9', 'std300_dema9','std300_D1dema9']\n",
    "preds = pd.DataFrame()\n",
    "for n in prediction_ind:\n",
    "    preds[n+\"_norm\"] = MaxMinRollNorm(data[n], 24*250)\n",
    "preds.tail()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     D1dema9_norm  std9_dema9_norm  std9_D1dema9_norm  \\\n",
       "Date                                                                    \n",
       "2021-09-30 19:00:00      0.549541         0.035342           0.157625   \n",
       "2021-09-30 20:00:00      0.509068         0.031738           0.166833   \n",
       "2021-09-30 21:00:00      0.593398         0.035424           0.145746   \n",
       "2021-09-30 22:00:00      0.599056         0.033126           0.123020   \n",
       "2021-09-30 23:00:00      0.585908         0.033670           0.104754   \n",
       "\n",
       "                     std100_dema9_norm  std100_D1dema9_norm  \\\n",
       "Date                                                          \n",
       "2021-09-30 19:00:00           0.417056             0.216130   \n",
       "2021-09-30 20:00:00           0.421059             0.216888   \n",
       "2021-09-30 21:00:00           0.424265             0.215617   \n",
       "2021-09-30 22:00:00           0.426708             0.215817   \n",
       "2021-09-30 23:00:00           0.428453             0.214252   \n",
       "\n",
       "                     std300_dema9_norm  std300_D1dema9_norm  \n",
       "Date                                                         \n",
       "2021-09-30 19:00:00           0.387081             0.228713  \n",
       "2021-09-30 20:00:00           0.390208             0.211806  \n",
       "2021-09-30 21:00:00           0.392990             0.210738  \n",
       "2021-09-30 22:00:00           0.395560             0.211329  \n",
       "2021-09-30 23:00:00           0.397978             0.211484  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1dema9_norm</th>\n",
       "      <th>std9_dema9_norm</th>\n",
       "      <th>std9_D1dema9_norm</th>\n",
       "      <th>std100_dema9_norm</th>\n",
       "      <th>std100_D1dema9_norm</th>\n",
       "      <th>std300_dema9_norm</th>\n",
       "      <th>std300_D1dema9_norm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-09-30 19:00:00</th>\n",
       "      <td>0.549541</td>\n",
       "      <td>0.035342</td>\n",
       "      <td>0.157625</td>\n",
       "      <td>0.417056</td>\n",
       "      <td>0.216130</td>\n",
       "      <td>0.387081</td>\n",
       "      <td>0.228713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-30 20:00:00</th>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>0.166833</td>\n",
       "      <td>0.421059</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>0.390208</td>\n",
       "      <td>0.211806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-30 21:00:00</th>\n",
       "      <td>0.593398</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>0.145746</td>\n",
       "      <td>0.424265</td>\n",
       "      <td>0.215617</td>\n",
       "      <td>0.392990</td>\n",
       "      <td>0.210738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-30 22:00:00</th>\n",
       "      <td>0.599056</td>\n",
       "      <td>0.033126</td>\n",
       "      <td>0.123020</td>\n",
       "      <td>0.426708</td>\n",
       "      <td>0.215817</td>\n",
       "      <td>0.395560</td>\n",
       "      <td>0.211329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-30 23:00:00</th>\n",
       "      <td>0.585908</td>\n",
       "      <td>0.033670</td>\n",
       "      <td>0.104754</td>\n",
       "      <td>0.428453</td>\n",
       "      <td>0.214252</td>\n",
       "      <td>0.397978</td>\n",
       "      <td>0.211484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot(preds['D1dema9_norm'].iloc[:]) # might consider to only divide, not subtract minimum as direction might be useful?\n",
    "#plt.show()\n",
    "#plt.plot(preds['std9_dema9_norm'].iloc[-500:])\n",
    "#plt.show()\n",
    "#plt.plot(preds['std100_dema9_norm'].iloc[-1000:])\n",
    "#plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "raw =  preds.dropna().to_numpy()\n",
    "wind = sliding_window_view(raw,window_shape = 3*5*24, axis=0)\n",
    "#print(wind[-1,:])\n",
    "print(wind.shape)\n",
    "#print(wind[~pd.isna(wind)].shape)\n",
    "x = wind[:-1,:]\n",
    "target_column = \"std100_dema9_norm\"\n",
    "y = wind[1:,3,-1]\n",
    "print(x.shape,y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(66390, 7, 360)\n",
      "(66389, 7, 360) (66389,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "train_x = torch.Tensor(x[0:int(len(x)*0.7),])\n",
    "train_y = torch.Tensor(y[0:int(len(x)*0.7)]).unsqueeze(1)\n",
    "test_x  = torch.Tensor(x[int(len(x)*0.7):,])\n",
    "test_y  = torch.Tensor(y[int(len(x)*0.7):]).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(torch.Tensor(x[0:int(len(x)*0.7),]),torch.Tensor(y[0:int(len(x)*0.7)]).unsqueeze(1) )\n",
    "test_dataset =  TensorDataset(torch.Tensor(x[int(len(x)*0.7):,]), torch.Tensor(y[int(len(x)*0.7):]).unsqueeze(1) )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                            batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                            batch_size=32, shuffle=True)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_31631/2941146206.py:3: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180507909/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  train_x = torch.Tensor(x[0:int(len(x)*0.7),])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from Models.neural import STDConvModel\n",
    "model = STDConvModel(3*5*24, 7, 12,stride1=2,stride2=2)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = model.to(device)\n",
    "class reverseKLD(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for creating a custom loss function, if desired.\n",
    "    If you instead specify a standard loss function,\n",
    "    you can remove or comment out this class.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(reverseKLD, self).__init__()\n",
    "        self.loss = torch.nn.KLDivLoss()\n",
    "    def forward(self, output, target):\n",
    "        #return torch.mean((output - target)**2)\n",
    "        return self.loss(target,output)\n",
    "criterion = reverseKLD()\n",
    "optimiser = optim.Adam(net.parameters())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def test_network(net,testloader, criterion):\n",
    "    net.eval()\n",
    "    total_images = 0\n",
    "    #total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "    #        _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(output,labels)\n",
    "            total_images += labels.size(0)\n",
    "    #        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    #print('      Accuracy on {0} test images: {1:.2f}%'.format(\n",
    "    #                            total_images, model_accuracy))\n",
    "    print(\"Test Loss: \", loss)\n",
    "    net.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(\"Start training...\")\n",
    "for epoch in range(0,1):\n",
    "    total_loss = 0\n",
    "    total_timeframes = 0\n",
    "    #total_correct = 0\n",
    "    i = 0\n",
    "    for batch in trainloader:           # Load batch\n",
    "        batch_x, batch_y = batch \n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        preds = net(batch_x)             # Process batch\n",
    "        #print(preds.shape,batch_y.shape)\n",
    "        loss = criterion(preds, batch_y) # Calculate loss\n",
    "        #print(\"Epoch:\",epoch,\" Batch:\",i,\" Loss:\",loss)\n",
    "        f = open(\"logs.txt\", \"a\")\n",
    "        f.write(\"Epoch:\"+str(epoch)+\" Batch:\"+str(i)+\" Loss:\"+str(loss))\n",
    "        f.write(str([x for x in net.parameters()]))\n",
    "        f.close()\n",
    "        #print([x for x in net.parameters()])\n",
    "        i += 1\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimiser.step()                # Update weights\n",
    "\n",
    "        output = preds.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_timeframes += batch_y.size(0)\n",
    "        #total_correct += output.eq(labels).sum().item()\n",
    "    #scheduler.step()\n",
    "    #model_accuracy = total_correct / total_images * 100\n",
    "    print('epoch {0} total_correct: {1} loss: {2:.2f} acc: {3:.2f}'.format(\n",
    "                epoch,-1, total_loss, -1) )\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    #    #if student.train_val_split < 1:\n",
    "    #    test_network(net,testloader)\n",
    "    #    torch.save(net.state_dict(), 'checkModel.pth')\n",
    "    #    print(\"      Model saved to checkModel.pth\")\n",
    "    #    elapsed = time.time() - t\n",
    "    #    print(elapsed)\n",
    "    \n",
    "#if student.train_val_split < 1:\n",
    "#    test_network(net,testloader)\n",
    "#torch.save(net.state_dict(), 'savedModel.pth')\n",
    "#print(\"   Model saved to savedModel.pth\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/sean/miniconda3/envs/qttrade/lib/python3.9/site-packages/torch/nn/functional.py:2747: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31631/1665620340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" Batch:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" Loss:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print([x for x in net.parameters()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "#plt.plot(test_y)\n",
    "#plt.show()\n",
    "#plt.plot(net(test_x).detach().numpy())\n",
    "\n",
    "print(test_y.shape)\n",
    "#print(net(test_x).detach().numpy().shape)\n",
    "#print(net(test_x).detach().numpy()[-20:])\n",
    "#print(nn.BatchNorm1d(net.num_features+1)(nn.ReLU()(net.conv1d(test_x[-1:,]))))\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=10_000)\n",
    "print(test_x[-1:,].shape)\n",
    "print(test_x[-1:,:,0])\n",
    "print(test_x[-1:,:,-1])\n",
    "print(net.conv1d)\n",
    "print(net.conv1d(test_x[-1:,]).shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([19917, 1])\n",
      "torch.Size([1, 7, 360])\n",
      "tensor([[0.6035, 0.0265, 0.0762, 0.1438, 0.0334, 0.1990, 0.0094]])\n",
      "tensor([[0.5991, 0.0331, 0.1230, 0.4267, 0.2158, 0.3956, 0.2113]])\n",
      "Conv1d(7, 8, kernel_size=(12,), stride=(2,))\n",
      "torch.Size([1, 8, 175])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stub_x = torch.rand ((1,7,360))\n",
    "print(stub_x[:,:,0])\n",
    "print(stub_x[:,:,-1])\n",
    "print(net.conv1d)\n",
    "print(net.conv1d(stub_x))\n",
    "newconv = nn.Conv1d(in_channels=7,out_channels=8,kernel_size=12,stride=2).to(device)\n",
    "print(newconv)\n",
    "print(newconv(stub_x))\n",
    "print(net.conv1d.weight.data)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.9953, 0.1908, 0.7067, 0.2966, 0.3359, 0.3151, 0.0053]])\n",
      "tensor([[0.2340, 0.2600, 0.5251, 0.7886, 0.3409, 0.1648, 0.1840]])\n",
      "Conv1d(7, 8, kernel_size=(12,), stride=(2,))\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<SqueezeBackward1>)\n",
      "Conv1d(7, 8, kernel_size=(12,), stride=(2,))\n",
      "tensor([[[ 0.5666,  0.4459,  0.0903,  ..., -0.0098,  0.4224,  0.2093],\n",
      "         [-0.0355, -0.3532, -0.0667,  ...,  0.0538, -0.3279, -0.1290],\n",
      "         [ 0.1829, -0.0448,  0.0867,  ..., -0.3430,  0.0703, -0.1879],\n",
      "         ...,\n",
      "         [ 0.3245,  0.3955,  0.3993,  ...,  0.3970,  0.4402,  0.1104],\n",
      "         [-0.0392, -0.0503, -0.2970,  ..., -0.0571, -0.0177, -0.3347],\n",
      "         [-0.2154,  0.0159, -0.2062,  ..., -0.2505, -0.1408, -0.2374]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([[[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from Models.neural import STDConvModel\n",
    "m1 = STDConvModel(3*5*24, 2, 12,stride1=2,stride2=2)\n",
    "m2 = STDConvModel(3*5*24, 2, 12,stride1=1,stride2=1)\n",
    "from torchinfo import summary\n",
    "print(m1(torch.zeros((20,2,5*24))).shape)\n",
    "#print(summary(m1,(20,2,5*24)))\n",
    "#print(summary(m2,(20,2,5*24)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('qttrade': conda)"
  },
  "interpreter": {
   "hash": "a67e17c8fc420817cbdbd0caa16d6c1c40b59f1d67d5d3254ca561366ced5b2e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}